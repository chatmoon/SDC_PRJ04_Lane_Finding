{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writeup | Advanced Lane Finding\n",
    "---\n",
    "[![Udacity](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract â€” This notebook is the writeup of the Advanced Lane Finding project.** We apply computer vision techniques using ```OpenCV functions``` to the task of lane finding as part of the SELF-DRIVING CAR nanodegree program. The project is broken down into eight steps, which are:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images\n",
    "* Apply a distortion correction to raw images\n",
    "* Use color transforms, gradients, etc, to create a thresholded binary image\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\")\n",
    "* Detect lane pixels and fit to find the lane boundary\n",
    "* Determine the curvature of the lane and vehicle position with respect to center\n",
    "* Warp the detected lane boundaries back onto the original image\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera calibration\n",
    "---\n",
    "*The code for this part is in **step1.py** file into the **module** folder. Look at the ```camera_calibrate()``` function.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a video as sources of images. But the images are distorted. We need to calibrate the camera to perform accurate measurements. To that end, we estimate the parameters defining the camera model, which are the camera calibration matrix, ```mtx```, and the distortion coefficients, ```dist```.\n",
    "\n",
    "First, we take pictures of chessboard shape with the camera to calibrate, then we detect and correct any distortion errors. To do this, I prepare ```object points``` which are points in ( x, y, z ) coordinate space of the chessboard corners in real world. x and y are horizontal and vertical indices of the chessboard corners. We assume the chessboard is a flat plane, then z is always 0.   \n",
    "\n",
    "\n",
    "```python\n",
    "# --- camera_calibrate(), lines 27-29 ---\n",
    "# prepare object points, like (0,0,0) , (1,0,0) , (2,0,0) ..., (7,5,0)\n",
    "objp = np.zeros((nx*ny, 3), np.float32)\n",
    "objp[:, :2] = np.mgrid[0:nx, 0:ny].T.reshape(-1, 2)  # x, y coordinates\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert RGB images to grayscale, then we use ```cv2.findChessboardCorners()``` to find and gather corners coordinates of each chessboard.\n",
    "```python\n",
    "# --- camera_calibrate(), lines 34-42 ---\n",
    "# convert frame to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "# find the chessboard corners\n",
    "ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "# if corners are found, add object points, frame points\n",
    "if ret == True:\n",
    "    camera['imgpoints'].append(corners)\n",
    "    camera['objpoints'].append(objp)\n",
    "```\n",
    "\n",
    "Thus, `objp` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follow picture shows the corners coordinates found in each chessboard.   \n",
    "\n",
    "![fig.0](https://raw.githubusercontent.com/chatmoon/SDC_PRJ04_Lane_Finding/master/_1_wip/output_images/_002_writeup_camera%20calibration.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then used the output `objpoints` and `imgpoints` to compute the camera calibration matrix ```mtx``` and the distortion coefficients ```dist``` using ```cv2.calibrateCamera()```.   \n",
    "\n",
    "\n",
    "```python\n",
    "# --- camera_calibrate(), line 47 ---\n",
    "reprojection_error, camera['camera_matrix'], camera['coef_distorsion'], rvecs, tvecs =\n",
    "       cv2.calibrateCamera(camera['objpoints'], camera['imgpoints'], gray.shape[::-1], None, None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply a distortion correction to raw images\n",
    "---\n",
    "*The code is in **step1.py** file into the **module** folder. Look at the ```image_undistort()``` function.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, I applied this distortion correction to the test image using the `cv2.undistort()` function and obtained the result below.   \n",
    "\n",
    "```python\n",
    "# --- image_undistort(), line 71 ---\n",
    "img_undistorted = cv2.undistort(image, output['camera_matrix'], output['coef_distorsion'], None, output['camera_matrix'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig.1](https://raw.githubusercontent.com/chatmoon/SDC_PRJ04_Lane_Finding/master/_1_wip/output_images/_003_writeup_code_undistort%20images.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create a thresholded binary image\n",
    "---   \n",
    "*The code is in **step2.py** file into the **module** folder.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a combinaison of various gradients and color thresholds to generate a binary image.   \n",
    "\n",
    "\n",
    "### Gradient absolute value\n",
    "\n",
    "We apply the $Sobel_{x}$ and $Sobel_{y}$ operators to each frame using ```cv2.Sobel()```. And then we apply a threshold on it.\n",
    "\n",
    "```python\n",
    "# --- sobel_xy(), sobel_abs(), scale(), mask(), gradient_sobel_abs(), lines 17-42 ---\n",
    "sobel_x     = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3))\n",
    "sobel_scale = np.uint8(thresh[1] * sobel_x / np.max(sobel_x))\n",
    "x_binary      = (sobel_scale >= thresh[0]) & (sobel_scale <= thresh[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient magnitude\n",
    "In addition, we can apply the gradient magnitude and a threshold on it.   \n",
    "Although it was implemented, I have got a better result without it.\n",
    "\n",
    "```python\n",
    "# --- gradient_magnitude(), lines 44-51 ---\n",
    "sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "gradmag = np.uint8(thresh[1] * np.hypot(sobel_x, sobel_y) / np.max(np.hypot(sobel_x, sobel_y)))\n",
    "result  = (gradmag >= thresh[0]) & (gradmag <= thresh[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient direction\n",
    "Furthermore, we can apply the gradient direction and a threshold on it.   \n",
    "Although it was implemented, I have got a better result without it.\n",
    "\n",
    "```python\n",
    "# --- gradient_direction(), lines 53-59 ---  \n",
    "sobel_x    = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=15))\n",
    "sobel_y    = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=15))\n",
    "absgraddir = np.arctan2(np.absolute(sobel_x), np.absolute(sobel_y))\n",
    "result     = (absgraddir >= thresh[0]) & (absgraddir <= thresh[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following picture shows results of different combinaisons of gradient thresholds.   \n",
    "\n",
    "![fig.2](https://raw.githubusercontent.com/chatmoon/SDC_PRJ04_Lane_Finding/master/_1_wip/output_images/_004_writeup_show%20binaries%20for%20grayscale%20image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color transforms\n",
    "\n",
    "I have tried several channel extractions from various color spaces. And then I apply a threshold on them.   \n",
    "At the end, I have only kept the Red, Saturation and Value channels.\n",
    "\n",
    "```python\n",
    "# --- threshold_color_gray(), threshold_color_rgb(), threshold_color_hls(), threshold_color_hsv(), lines 61-92 ---\n",
    "rgb       = mpimg.imread('test.jpg')\n",
    "r_channel = rgb[:, :, 0]\n",
    "r_binary  = (r_channel >= thresh[0]) & (r_channel <= thresh[1])\n",
    "\n",
    "hls       = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "s_channel = hls[:, :, 2]\n",
    "s_binary  = (s_channel >= thresh[0]) & (s_channel <= thresh[1])\n",
    "\n",
    "hsv       = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "v_channel = hsv[:, :, index]\n",
    "v_binary  = (v_channel >= thresh[0]) & (v_channel <= thresh[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following picture shows results of these various channel extractions before and after the thresholding.\n",
    "\n",
    "![fig.3](https://raw.githubusercontent.com/chatmoon/SDC_PRJ04_Lane_Finding/master/_1_wip/output_images/_007_writeup_%20merge%20of%205%20and%206.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Combinaison of various gradients and  color thresholds\n",
    "\n",
    "Finally, I have choosen to combine gradient absolute value in both axis and color thresholds with Red, Saturation and Value channels.\n",
    "\n",
    "```python\n",
    "# --- combine_threshold(), lines 115-148 ---\n",
    "result[(x_binary & y_binary) | (r_binary  & s_binary & v_binary)] = 255\n",
    "```\n",
    "\n",
    "The table below shows the threshold values that have been used.   \n",
    "\n",
    "| Item         \t\t      |    Threshold Values   \t| \n",
    "|:-----------------------:|:-----------------------:| \n",
    "| Gradient absolute value | (20, 100)               |     \n",
    "| Red channel             | (200, 255)              |     \n",
    "| Saturation channel      | (100, 255)              |     \n",
    "| Value channel           | (50, 255)               |    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture below shows the final result.   \n",
    "\n",
    "![fig.4](https://raw.githubusercontent.com/chatmoon/SDC_PRJ04_Lane_Finding/master/_1_wip/output_images/_008_writeup_show%20combinaison%20of%20binaries.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective transform (\"birds-eye view\")\n",
    "---\n",
    "*The code for this part is in **step3.py** file into the **module** folder.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `image_warp()` function takes as inputs an image and  apart of source coordinates (`x_top = [ 594, 686 ]`).\n",
    "\n",
    "I chose to hardcode the destination and remaining source points coordinates in the following manner:\n",
    "```python\n",
    "# --- image_warp(), lines 18-25 ---\n",
    "# four source coordinates\n",
    "src = np.float32([ [x_top[0], 450], [x_top[1], 450], [1045, 665], [262, 665]])\n",
    "# four desired coordinates\n",
    "dst = np.float32([ [262, 100], [1045, 100]         , [1045, 665], [262, 665] ])   \n",
    "```   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resulted in the following source and destination points:\n",
    "\n",
    "| Source        | Destination   | \n",
    "|:-------------:|:-------------:| \n",
    "| 594, 450      | 262, 100      | \n",
    "| 262, 665      | 262, 665      |\n",
    "| 1045, 665     | 1045, 665     |\n",
    "| 686, 450      | 1045, 100     |   \n",
    "\n",
    "We use `cv2.getPerspectiveTransform()` to compute the perspective trasnform matrix, `M`. Then we transforme the source to the destination image using `cv2.warpPerspective()` with the linear interpolation option.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following picture shows the result of a perspective transform on straight lane lines.\n",
    "\n",
    "![fig.5](https://raw.githubusercontent.com/chatmoon/SDC_PRJ04_Lane_Finding/master/_1_wip/output_images/_009_writeup_perspective%20transform.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect lane pixels and fit to find the lane boundary\n",
    "---\n",
    "*The code for this part is in **tracker.py** file into the **module** folder.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the following general approach to dectecte lines pixels in the first frame :   \n",
    "1. locate the approximate location of lane lines using peaks of histogram: `find_histopeak()`\n",
    "2. start with windows at the bottom\n",
    "3. scan from bottom to top: `search_sliding_window()`\n",
    "4. find non-zero pixels within each window: `find_nonzero_pixels()`\n",
    "5. move the window upwards changing its location along the x axis adding +/- marging to the previous values: `search_sliding_window()`\n",
    "6. fit a second degree polynomial once we have the coordinates of the lane lines: `find_lanes_init()`\n",
    "7. use this polynomial to plot the lane lines from top to bottom in the warped image: `draw_lane()`\n",
    "\n",
    "For the next frames, we skip the sliding windows step once we know where the lines are and we search in a margin around the previous line: `find_lanes_next()`\n",
    "\n",
    "The following picture shows the result of the dectection of the lines pixels.\n",
    "\n",
    "![fig6](https://raw.githubusercontent.com/chatmoon/SDC_PRJ04_Lane_Finding/master/_1_wip/output_images/_010_writeup_find%20lane%20lines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have implemented some sanity checks, which are:\n",
    "- xxx\n",
    "- yyy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The curvature of the lane and vehicle position with respect to center\n",
    "---\n",
    "*The code for this part is in **tracker.py** file into the **module** folder.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, I located the lane line pixels, used their x and y pixel positions to fit a second order polynomial curve:   \n",
    "\n",
    "\\begin{equation*}\n",
    "f(y) = Ay^2 + By + C\n",
    "\\end{equation*}\n",
    "\n",
    "The radius of curvature at any point x of the function `x=f(y)` is given as follows:   \n",
    "\n",
    "\\begin{equation*}\n",
    "R_{curve} = \\frac{ ( 1 + (2Ay + B)^{2}  )^{3/2}      }{|2A|}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I calculated the radius of curvature at the base of the image:\n",
    "```python\n",
    "# --- curvature(), lines 155-158 ---\n",
    "def curvature(height, lane_x):\n",
    "    x, y = lane_x[:, 0], lane_x[:, 1]\n",
    "    coeff = np.polyfit(y * y_mx, x * x_mx, 2)\n",
    "    return ((1 + (2*coeff[0] *height *y_mx + coeff[1])** 2)** 1.5) / np.absolute(2 *coeff[0])\n",
    "```\n",
    "\n",
    "The pixel values are converted to meters with the following ratios:\n",
    "```python\n",
    "y_mx = 27 / 720  # meters per pixel in y dimension\n",
    "x_mx = 3.7 / 812 # meters per pixel in x dimension\n",
    "```\n",
    "\n",
    "The vehicle position is calculated at the bottom of the frame as the following:\n",
    "```python\n",
    "# --- camera_offset(), lines 159-166 ---\n",
    "def camera_offset(width, x_left, x_right):\n",
    "    # calculate the offset of the car on the road\n",
    "    center_lane = (x_left[-1] + x_right[-1]) / 2\n",
    "    center_diff = (center_lane - width / 2) * x_mx\n",
    "    side_pos    = 'left'\n",
    "    if center_diff <= 0:\n",
    "        side_pos = 'right'\n",
    "    return np.absolute(center_diff), side_pos\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warp the detected lane boundaries back onto the original image\n",
    "---\n",
    "*The code for this part is in **tracker.py** file into the **module** folder.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I warped the detected lane boundaries back onto the original image. I also add the output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# --- draw_lane(), lines 218-250 ---\n",
    "def draw_lane(image, left_fit, right_fit, Minv):\n",
    "    # image: width, height\n",
    "    (width, height) = reversed(image.shape[:2])\n",
    "    # extract lines\n",
    "    (x_left, x_right, y_lane), (left_lane, right_lane, inner_lane) = extract_line(image, left_fit, right_fit)\n",
    "    # draw the lane onto the image_warped blank image\n",
    "    road, road_bkg     = np.zeros_like(image), np.zeros_like(image)\n",
    "    cv2.fillPoly(road, np.int32([left_lane]), color=[255, 0, 0])\n",
    "    cv2.fillPoly(road, np.int32([right_lane]), color=[0, 0, 255])\n",
    "    cv2.fillPoly(road, np.int32([inner_lane]), color=[0, 255, 0])\n",
    "    cv2.fillPoly(road_bkg, np.int32([left_lane]), color=[255, 255, 255])\n",
    "    cv2.fillPoly(road_bkg, np.int32([right_lane]), color=[255, 255, 255])\n",
    "    road_warped = cv2.warpPerspective(road, Minv, (width, height), flags=cv2.INTER_LINEAR)\n",
    "    road_warped_bkg=cv2.warpPerspective(road_bkg, Minv,(width, height), flags=cv2.INTER_LINEAR)\n",
    "    base = cv2.addWeighted(image, 1.0, road_warped_bkg, -1.0, 0.0)  # 1.3, 0.0)\n",
    "    result = cv2.addWeighted(base, 1.0, road_warped, 0.7, 0.0)  # 1.3, 0.0)\n",
    "    # calculate the offset of the car on the road\n",
    "    center_diff, side_pos = camera_offset(width, x_left, x_right)\n",
    "    # draw the text showing curvature, offset, and speed\n",
    "    curverad = self.curvature(height, left_lane)\n",
    "    cv2.putText(result, 'Radius of Curvature = ' + str(int(curverad))+ ' m', (50, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.putText(result, 'Vehicle is ' + str(abs(round(center_diff, 3))) + ' m ' + side_pos \n",
    "                + ' of center', (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following picture shows the result.\n",
    "\n",
    "![fig.7](https://raw.githubusercontent.com/chatmoon/SDC_PRJ04_Lane_Finding/master/_1_wip/output_images/_011_writeup_warp%20back.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find here the video output.\n",
    "[![video0](https://img.youtube.com/vi/iWqL5C8fFyM/0.jpg)](https://www.youtube.com/watch?v=iWqL5C8fFyM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
